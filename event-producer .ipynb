{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57cf458-0459-420c-ac8d-766788668166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-eventhub\n  Downloading azure_eventhub-5.15.0-py3-none-any.whl.metadata (73 kB)\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/73.1 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/73.1 kB\u001B[0m \u001B[31m1.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m71.7/73.1 kB\u001B[0m \u001B[31m908.4 kB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.1/73.1 kB\u001B[0m \u001B[31m785.1 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hRequirement already satisfied: azure-core>=1.27.0 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (1.31.0)\nRequirement already satisfied: typing-extensions>=4.0.1 in /databricks/python3/lib/python3.12/site-packages (from azure-eventhub) (4.11.0)\nRequirement already satisfied: requests>=2.21.0 in /databricks/python3/lib/python3.12/site-packages (from azure-core>=1.27.0->azure-eventhub) (2.32.2)\nRequirement already satisfied: six>=1.11.0 in /usr/lib/python3/dist-packages (from azure-core>=1.27.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (1.26.16)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.27.0->azure-eventhub) (2024.6.2)\nDownloading azure_eventhub-5.15.0-py3-none-any.whl (327 kB)\n\u001B[?25l   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/327.8 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m112.6/327.8 kB\u001B[0m \u001B[31m4.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m204.8/327.8 kB\u001B[0m \u001B[31m3.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m327.8/327.8 kB\u001B[0m \u001B[31m3.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: azure-eventhub\nSuccessfully installed azure-eventhub-5.15.0\n\u001B[43mNote: you may need to restart the kernel using %restart_python or dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# Installing Event Hubs Library\n",
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9efdd46-da9b-447d-a014-0b24d01fd05d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Restarting the kernal to see updated changes\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f45dc73c-8214-4431-8fbd-d85f68312368",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Principal configured for ADLS access.\nTesting connection to abfss://raw@adlsshivendra.dfs.core.windows.net/...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/ bronze_load_config.json</td><td> bronze_load_config.json</td><td>662</td><td>1758434032000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/dim_customers.csv</td><td>dim_customers.csv</td><td>591879</td><td>1758383243000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/dim_products.csv</td><td>dim_products.csv</td><td>19660</td><td>1758383234000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/dim_stores.csv</td><td>dim_stores.csv</td><td>593</td><td>1758383234000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/fact_orders_daily_batch.csv</td><td>fact_orders_daily_batch.csv</td><td>2649748</td><td>1758383259000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/inventory_snapshot_daily.csv</td><td>inventory_snapshot_daily.csv</td><td>3167185</td><td>1758383267000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/payments.csv</td><td>payments.csv</td><td>1918878</td><td>1758383250000</td></tr><tr><td>abfss://raw@adlsshivendra.dfs.core.windows.net/stream_events.ndjson</td><td>stream_events.ndjson</td><td>429340</td><td>1758383241000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/ bronze_load_config.json",
         " bronze_load_config.json",
         662,
         1758434032000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/dim_customers.csv",
         "dim_customers.csv",
         591879,
         1758383243000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/dim_products.csv",
         "dim_products.csv",
         19660,
         1758383234000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/dim_stores.csv",
         "dim_stores.csv",
         593,
         1758383234000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/fact_orders_daily_batch.csv",
         "fact_orders_daily_batch.csv",
         2649748,
         1758383259000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/inventory_snapshot_daily.csv",
         "inventory_snapshot_daily.csv",
         3167185,
         1758383267000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/payments.csv",
         "payments.csv",
         1918878,
         1758383250000
        ],
        [
         "abfss://raw@adlsshivendra.dfs.core.windows.net/stream_events.ndjson",
         "stream_events.ndjson",
         429340,
         1758383241000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection test successful. You should see your 'stream_events.ndjson' file listed above.\n"
     ]
    }
   ],
   "source": [
    "# Configuring Service Principal \n",
    "\n",
    "STORAGE_ACCOUNT_NAME = \"adlsshivendra\"\n",
    "ADLS_CONTAINER_NAME = \"raw\"\n",
    "CLIENT_ID = \"4069d864-2ea2-4354-a634-71a8488be89e\"\n",
    "TENANT_ID = \"4ac50105-0c66-404e-a107-7cbd8a9a6442\"\n",
    "CLIENT_SECRET = \"o.O8Q~ARpL1hfZzcjC_yJZTPhhc-jTCDlI_BRdif\"\n",
    "\n",
    "# Spark Configuration \n",
    "spark.conf.set(f\"fs.azure.account.auth.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth.provider.type.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.id.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", CLIENT_ID)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.secret.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", CLIENT_SECRET)\n",
    "spark.conf.set(f\"fs.azure.account.oauth2.client.endpoint.{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{TENANT_ID}/oauth2/token\")\n",
    "\n",
    "print(\"Service Principal configured for ADLS access.\")\n",
    "\n",
    "#Testing the connection\n",
    "try:\n",
    "    print(f\"Testing connection to abfss://{ADLS_CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/...\")\n",
    "    # This will list the files in your 'raw' container\n",
    "    display(dbutils.fs.ls(f\"abfss://{ADLS_CONTAINER_NAME}@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/\"))\n",
    "    print(\"Connection test successful. You should see your 'stream_events.ndjson' file listed above.\")\n",
    "except Exception as e:\n",
    "    print(f\"Connection test FAILED. Check your Service Principal IAM role ('Storage Blob Data Contributor').\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09987f11-199c-4875-871c-278284bc3c7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading events from abfss://raw@adlsshivendra.dfs.core.windows.net/stream_events.ndjson...\nFound 2160 events to send.\nConnecting to Event Hub and sending events...\n------------------------------\nAll 2160 events sent successfully.\n"
     ]
    }
   ],
   "source": [
    "# Producer Script\n",
    "from azure.eventhub import EventHubProducerClient, EventData\n",
    "\n",
    "# 1. Getting the Event Hub connection string from secrets\n",
    "EH_CONN_STR = \"Endpoint=sb://capstonenamespace.servicebus.windows.net/;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=qruC209PZzuwwjWAavde4bJI3mPblMCrN+AEhB1XUjw=;EntityPath=retail-events\"\n",
    "\n",
    "EH_NAME = \"retail-events\"\n",
    "\n",
    "# Defining the source file in ADLS\n",
    "source_path = f\"abfss://raw@{STORAGE_ACCOUNT_NAME}.dfs.core.windows.net/stream_events.ndjson\"\n",
    "\n",
    "# Reading the NDJSON file into a list\n",
    "print(f\"Reading events from {source_path}...\")\n",
    "try:\n",
    "    events = spark.read.text(source_path).collect() \n",
    "    print(f\"Found {len(events)} events to send.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from ADLS. Check Cell 2 config and file path.\")\n",
    "    raise e\n",
    "\n",
    "# Creating the producer client\n",
    "producer = EventHubProducerClient.from_connection_string(EH_CONN_STR, eventhub_name=EH_NAME)\n",
    "\n",
    "# 5. Sending the events in batches\n",
    "print(\"Connecting to Event Hub and sending events...\")\n",
    "batch = producer.create_batch()\n",
    "total_sent = 0\n",
    "\n",
    "for i, event_row in enumerate(events):\n",
    "    try:\n",
    "        # Add the event (which is a JSON string) to the batch\n",
    "        batch.add(EventData(event_row.value))\n",
    "        total_sent += 1\n",
    "    except ValueError:\n",
    "        # Batch is full, send it and create a new one\n",
    "        producer.send_batch(batch)\n",
    "        print(f\"  ...sent batch {i // 100}...\")\n",
    "        batch = producer.create_batch()\n",
    "        batch.add(EventData(event_row.value))\n",
    "\n",
    "# Sending any remaining events in the last batch\n",
    "if len(batch) > 0:\n",
    "    producer.send_batch(batch)\n",
    "    \n",
    "producer.close()\n",
    "print(\"-\" * 30)\n",
    "print(f\"All {total_sent} events sent successfully.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "event-producer",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}